{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78cd8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['work_dirs/host_hybrid/102/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/104/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/106/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/1099/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/11/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/110/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/1119/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/1365/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/145/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/1552/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/1585/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/1661/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/191/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/192/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/199/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2066/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/21/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2110/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2113/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2132/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/216/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2192/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2205/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2215/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/224/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2250/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2290/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2296/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/23/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2443/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/257/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/2916/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/296/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3177/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/321/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3210/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3773/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3826/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3950/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3963/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3969/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/3974/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4014/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4069/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4093/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4114/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4115/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4129/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4135/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4155/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4170/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4178/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4268/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4305/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4631/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/4934/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/5833/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/6076/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/6134/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/6208/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/6932/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/70/eq1_final.data',\n",
       " 'work_dirs/host_hybrid/7186/eq1_final.data']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "print(len(glob.glob('work_dirs/host_hybrid/*/eq1_final.data')))\n",
    "sorted(glob.glob('work_dirs/host_hybrid/*/eq1_final.data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e48729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*CCCS(*)(=O)=O'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "all_smiles = pl.read_csv('../data/from_host/train.csv')['SMILES'].to_list()\n",
    "SMILES_INDEX = 1552\n",
    "all_smiles[SMILES_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b18d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] n_atoms=5960, avg_mass_amu=8.777, density_g_per_cm3=0.957941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/MDAnalysis/topology/LAMMPSParser.py:651: UserWarning: Guessed all Masses to 1.0\n",
      "  warnings.warn(\"Guessed all Masses to 1.0\")\n",
      "/home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/MDAnalysis/coordinates/LAMMPS.py:751: UserWarning: Reader has no dt information, set to 1.0 ps\n",
      "  ts.data[\"time\"] = step_num * ts.dt\n"
     ]
    }
   ],
   "source": [
    "# Python >= 3.11, Ubuntu. Verbose names, single spaces around equals.\n",
    "import re\n",
    "import numpy as np\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.lib import mdamath\n",
    "\n",
    "def parse_type_to_mass_map_from_lammps_data(data_file_path: str) -> dict[int, float]:\n",
    "    \"\"\"\n",
    "    Parse the 'Masses' section of a LAMMPS DATA file and return {atom_type: mass_amu}.\n",
    "    Robust to comments; stops when the next section header begins.\n",
    "    \"\"\"\n",
    "    with open(data_file_path, \"r\") as file_handle:\n",
    "        all_lines = file_handle.read().splitlines()\n",
    "\n",
    "    start_index = None\n",
    "    for index, line_text in enumerate(all_lines):\n",
    "        if line_text.strip().lower().startswith(\"masses\"):\n",
    "            start_index = index + 1\n",
    "            break\n",
    "    if start_index is None:\n",
    "        raise ValueError(\"No 'Masses' section found in DATA file.\")\n",
    "\n",
    "    type_to_mass_map: dict[int, float] = {}\n",
    "    numeric_line_pattern = re.compile(\n",
    "        r\"^\\s*(\\d+)\\s+([+\\-]?(?:\\d+\\.?\\d*|\\.\\d+)(?:[eE][+\\-]?\\d+)?)\"\n",
    "    )\n",
    "\n",
    "    i = start_index\n",
    "    while i < len(all_lines):\n",
    "        text = all_lines[i].strip()\n",
    "        i += 1\n",
    "        if not text:\n",
    "            continue\n",
    "        if text[0].isalpha():  # next section header like \"Atoms\", \"Bonds\", etc.\n",
    "            break\n",
    "        m = numeric_line_pattern.match(text)\n",
    "        if m:\n",
    "            atom_type = int(m.group(1))\n",
    "            mass_amu = float(m.group(2))\n",
    "            type_to_mass_map[atom_type] = mass_amu\n",
    "\n",
    "    if not type_to_mass_map:\n",
    "        raise ValueError(\"Failed to parse any masses from the 'Masses' section.\")\n",
    "    return type_to_mass_map\n",
    "\n",
    "def initialize_universe_with_dump_and_masses(\n",
    "    lammps_dump_path: str,\n",
    "    lammps_data_path_for_masses: str,\n",
    ") -> mda.Universe:\n",
    "    \"\"\"\n",
    "    Load coordinates/box/types from a LAMMPS dump, attach masses from a DATA file's 'Masses' section,\n",
    "    and return a ready-to-use MDAnalysis Universe.\n",
    "    \"\"\"\n",
    "    # 1) Load the dump (coordinates + periodic box + atom types)\n",
    "    universe = mda.Universe(lammps_dump_path, format=\"LAMMPSDUMP\")\n",
    "\n",
    "    # 2) Parse type->mass (amu) from the DATA file\n",
    "    type_to_mass_amu = parse_type_to_mass_map_from_lammps_data(lammps_data_path_for_masses)\n",
    "\n",
    "    # 3) Build per-atom masses from 'types' in the dump and attach to Universe\n",
    "    atom_types_raw = np.asarray(universe.atoms.types)\n",
    "    try:\n",
    "        atom_types_int = atom_types_raw.astype(int)\n",
    "    except Exception:\n",
    "        atom_types_int = np.array([int(str(t)) for t in atom_types_raw], dtype=int)\n",
    "\n",
    "    missing_types = sorted(set(atom_types_int) - set(type_to_mass_amu.keys()))\n",
    "    if missing_types:\n",
    "        raise KeyError(f\"Missing masses for atom types: {missing_types}\")\n",
    "\n",
    "    per_atom_masses_amu = np.array([type_to_mass_amu[t] for t in atom_types_int], dtype=float)\n",
    "    universe.add_TopologyAttr(\"masses\", per_atom_masses_amu)\n",
    "\n",
    "    # 4) Optional sanity checks (safe bounds: average atomic mass between ~6 and ~40 amu)\n",
    "    average_atomic_mass_amu = float(per_atom_masses_amu.mean())\n",
    "    if not (5.0 <= average_atomic_mass_amu <= 40.0):\n",
    "        raise RuntimeError(f\"Average atomic mass looks off: {average_atomic_mass_amu:.2f} amu\")\n",
    "\n",
    "    # 5) Optional: quick density print for verification (g/cm^3)\n",
    "    amu_to_g = 1.66053906660e-24\n",
    "    volume_angstrom3 = mdamath.box_volume(universe.dimensions)  # Ã…^3\n",
    "    density_g_per_cm3 = (per_atom_masses_amu.sum() * amu_to_g) / (volume_angstrom3 * 1e-24)\n",
    "    print(f\"[init] n_atoms={len(universe.atoms)}, avg_mass_amu={average_atomic_mass_amu:.3f}, \"\n",
    "          f\"density_g_per_cm3={density_g_per_cm3:.6f}\")\n",
    "\n",
    "    return universe\n",
    "\n",
    "# -------------------------\n",
    "# Example usage:\n",
    "# -------------------------\n",
    "dump_path = f\"work_dirs/host_hybrid/{SMILES_INDEX}/eq1_final.dump\"\n",
    "data_path = f\"work_dirs/host_hybrid/{SMILES_INDEX}/eq1_final.data\"\n",
    "u = initialize_universe_with_dump_and_masses(\n",
    "    lammps_dump_path=dump_path,\n",
    "    lammps_data_path_for_masses=data_path,\n",
    ")\n",
    "\n",
    "# topology_path = \"work_dirs/host_hybrid/70/eq1_final.data\"\n",
    "# coords_path   = \"work_dirs/host_hybrid/70/eq1_final.dump\"\n",
    "# u = mda.Universe(\n",
    "#     topology_path,\n",
    "#     coords_path,\n",
    "#     format=(\"LAMMPSDATA\", \"LAMMPSDUMP\"),\n",
    "#     atom_style=\"full\",\n",
    "#     n_atoms=count_atoms_from_data_file(topology_path),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35f5378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached bonds: 5950\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def parse_bond_atom_ids_from_lammps_data(data_file_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns an (N, 2) array of LAMMPS atom IDs (1-based) from the DATA file's 'Bonds' section.\n",
    "    Assumes canonical 'Bonds' lines:  id  type  atom_i  atom_j  [# comment]\n",
    "    \"\"\"\n",
    "    with open(data_file_path, \"r\") as fh:\n",
    "        lines = fh.read().splitlines()\n",
    "\n",
    "    # locate 'Bonds' header\n",
    "    start = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().lower().startswith(\"bonds\"):\n",
    "            start = i + 1\n",
    "            break\n",
    "    if start is None:\n",
    "        raise ValueError(\"No 'Bonds' section found in DATA file.\")\n",
    "\n",
    "    bonds = []\n",
    "    patt = re.compile(r\"^\\s*(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\")\n",
    "    i = start\n",
    "    while i < len(lines):\n",
    "        s = lines[i].strip()\n",
    "        i += 1\n",
    "        if not s:\n",
    "            continue\n",
    "        if s[0].isalpha():  # next section header\n",
    "            break\n",
    "        m = patt.match(s)\n",
    "        if m:\n",
    "            ai_id = int(m.group(3))  # LAMMPS atom IDs (1-based)\n",
    "            aj_id = int(m.group(4))\n",
    "            bonds.append((ai_id, aj_id))\n",
    "    if not bonds:\n",
    "        raise ValueError(\"Parsed zero bonds from 'Bonds' section.\")\n",
    "    return np.asarray(bonds, dtype=int)\n",
    "\n",
    "def attach_bonds_by_atom_ids(universe, bonds_atom_ids_1based: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Map (atom_id_i, atom_id_j) to 0-based indices in the current Universe and attach as 'bonds'.\n",
    "    \"\"\"\n",
    "    # Build ID -> index map from the dump (your dump includes 'id', so MDAnalysis populated .ids)\n",
    "    id_to_index = {int(atom_id): idx for idx, atom_id in enumerate(universe.atoms.ids)}\n",
    "    pairs = []\n",
    "    for ai_id, aj_id in bonds_atom_ids_1based:\n",
    "        if ai_id in id_to_index and aj_id in id_to_index:\n",
    "            ai = id_to_index[ai_id]\n",
    "            aj = id_to_index[aj_id]\n",
    "            if ai != aj:\n",
    "                pairs.append((ai, aj))\n",
    "    if not pairs:\n",
    "        raise RuntimeError(\"No bonds matched current atom IDs; check that DATA and dump refer to the same system.\")\n",
    "    bonds_indices = np.asarray(pairs, dtype=int)\n",
    "    # If bonds already exist, skip; otherwise attach\n",
    "    if getattr(universe, \"bonds\", None) is None or len(universe.bonds) == 0:\n",
    "        universe.add_TopologyAttr(\"bonds\", bonds_indices)\n",
    "\n",
    "# ---- call this once after you create `u` from the dump/xtc ----\n",
    "bonds_ids = parse_bond_atom_ids_from_lammps_data(data_path)\n",
    "attach_bonds_by_atom_ids(u, bonds_ids)\n",
    "\n",
    "print(f\"Attached bonds: {len(u.bonds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9702791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho = 0.958, Q6 = 0.0085, gr_peak = 1.08 Ã…\n",
      "Voronoi volumes: mean = 15.21 Ã…^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from MDAnalysis.lib import mdamath\n",
    "import freud\n",
    "\n",
    "# --- positions & box for freud ---\n",
    "positions_angstrom = u.atoms.positions.astype(np.float64)\n",
    "box_matrix_angstrom = mdamath.triclinic_vectors(u.dimensions)  # 3x3\n",
    "freud_box = freud.Box.from_matrix(box_matrix_angstrom)\n",
    "\n",
    "# --- mass density (g/cm^3) ---\n",
    "amu_to_g = 1.66053906660e-24\n",
    "volume_ang3 = mdamath.box_volume(u.dimensions)          # Ã…^3\n",
    "rho_g_cm3 = (u.atoms.masses.sum() * amu_to_g) / (volume_ang3 * 1e-24)\n",
    "\n",
    "# --- RDF first peak ---\n",
    "rdf = freud.density.RDF(bins=200, r_max=10.0)\n",
    "rdf.compute(system=(freud_box, positions_angstrom))\n",
    "first_peak_idx = int(np.argmax(rdf.rdf))\n",
    "first_peak_distance_angstrom = float(rdf.bin_centers[first_peak_idx])\n",
    "\n",
    "# --- Steinhardt Q6 ---\n",
    "q6 = freud.order.Steinhardt(l=6)\n",
    "q6.compute((freud_box, positions_angstrom), neighbors={'num_neighbors': 12, 'exclude_ii': True})\n",
    "average_q6 = float(q6.order)\n",
    "\n",
    "# --- Voronoi cell volumes (Ã…^3) ---\n",
    "voro = freud.locality.Voronoi()\n",
    "voro.compute(system=(freud_box, positions_angstrom))\n",
    "voronoi_volumes_ang3 = voro.volumes  # per-atom cell volumes\n",
    "\n",
    "print(f\"rho = {rho_g_cm3:.3f}, Q6 = {average_q6:.4f}, gr_peak = {first_peak_distance_angstrom:.2f} Ã…\")\n",
    "print(f\"Voronoi volumes: mean = {np.mean(voronoi_volumes_ang3):.2f} Ã…^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e36b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFV (monte-carlo): 0.3735080295138889\n"
     ]
    }
   ],
   "source": [
    "# --- Build Voronoi & volumes (you already have freud_box and positions_angstrom) ---\n",
    "voronoi_calculator = freud.locality.Voronoi()\n",
    "voronoi_calculator.compute(system=(freud_box, positions_angstrom))\n",
    "voronoi_volumes_ang3 = np.asarray(voronoi_calculator.volumes, dtype=float)  # per-atom\n",
    "\n",
    "# --- Choose van der Waals radii per atom (Ã…) ---\n",
    "# If you have element names: map element->radius; otherwise fallback per-type or a single value.\n",
    "bondi_vdw_radius_by_element_angstrom = {\n",
    "    \"H\": 1.20, \"C\": 1.70, \"N\": 1.55, \"O\": 1.52, \"F\": 1.47,\n",
    "    \"P\": 1.80, \"S\": 1.80, \"Cl\": 1.75, \"Br\": 1.85, \"Si\": 2.10\n",
    "}\n",
    "\n",
    "def build_per_atom_vdw_radii_angstrom(universe):\n",
    "    # Try to use elements if available; else namesâ€™ leading letters; else default heavy-atom radius.\n",
    "    try:\n",
    "        element_symbols = [el if el is not None else \"\" for el in getattr(universe.atoms, \"elements\", [None]*len(universe.atoms))]\n",
    "    except Exception:\n",
    "        element_symbols = [\"\" for _ in range(len(universe.atoms))]\n",
    "\n",
    "    per_atom_radii = []\n",
    "    for atom, element_symbol in zip(universe.atoms, element_symbols):\n",
    "        radius = None\n",
    "        key = element_symbol if element_symbol in bondi_vdw_radius_by_element_angstrom else str(getattr(atom, \"name\", \"\")).rstrip(\"0123456789\")\n",
    "        if key in bondi_vdw_radius_by_element_angstrom:\n",
    "            radius = bondi_vdw_radius_by_element_angstrom[key]\n",
    "        elif key and key[0] in bondi_vdw_radius_by_element_angstrom:\n",
    "            radius = bondi_vdw_radius_by_element_angstrom[key[0]]\n",
    "        else:\n",
    "            # Fallback: treat as generic heavy atom\n",
    "            radius = 1.70\n",
    "        per_atom_radii.append(radius)\n",
    "    return np.asarray(per_atom_radii, dtype=float)\n",
    "\n",
    "def estimate_ffv_monte_carlo(\n",
    "    freud_box,\n",
    "    atom_positions_angstrom: np.ndarray,\n",
    "    per_atom_vdw_radii_angstrom: np.ndarray,\n",
    "    probe_radius_angstrom: float = 0.0,\n",
    "    grid_points_per_axis: int = 48,\n",
    "    rng_seed: int = 0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns an estimate of the probe-accessible FFV using uniform grid sampling\n",
    "    with PBC. Increase grid_points_per_axis for accuracy (cost ~ N^3).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # --- build Cartesian grid in fractional space, then map to box ---\n",
    "    fractional_lin = (np.arange(grid_points_per_axis, dtype=float) + 0.5) / grid_points_per_axis\n",
    "    fx, fy, fz = np.meshgrid(fractional_lin, fractional_lin, fractional_lin, indexing=\"ij\")\n",
    "    sample_fracs = np.column_stack([fx.ravel(), fy.ravel(), fz.ravel()])  # (M,3)\n",
    "\n",
    "    box_matrix_angstrom = freud_box.to_matrix().astype(float)  # 3x3\n",
    "    sample_points_angstrom = sample_fracs @ box_matrix_angstrom.T         # (M,3)\n",
    "\n",
    "    # --- neighbor query with a single conservative cutoff (max radius) ---\n",
    "    max_effective_radius = float(np.max(per_atom_vdw_radii_angstrom) + probe_radius_angstrom)\n",
    "    neighbor_query = freud.locality.AABBQuery(freud_box, atom_positions_angstrom.astype(float))\n",
    "    neighbor_list = neighbor_query.query(sample_points_angstrom, {\"r_max\": max_effective_radius}).toNeighborList()\n",
    "\n",
    "    # --- mark any sample point that has at least one neighbor within that conservative cutoff as \"potentially occupied\" ---\n",
    "    # This is conservative; to be stricter, filter by per-atom specific radius below (optional refinement).\n",
    "    occupied_flags = np.zeros(sample_points_angstrom.shape[0], dtype=bool)\n",
    "    occupied_flags[np.asarray(neighbor_list.query_point_indices, dtype=np.int64)] = True\n",
    "\n",
    "    # Optional refinement: shrink back by checking per-atom specific radii\n",
    "    # (kept lightweight; comment out if the neighbor list exposes distances differently in your freud)\n",
    "    try:\n",
    "        # If your freud exposes neighbor vectors/distances, validate per-pair\n",
    "        neighbor_vectors = np.asarray(neighbor_list.separations, dtype=float)  # shape (K,3) in some freud builds\n",
    "        neighbor_distances = np.linalg.norm(neighbor_vectors, axis=1)\n",
    "        neighbor_atom_indices = np.asarray(neighbor_list.point_indices, dtype=np.int64)\n",
    "        neighbor_query_indices = np.asarray(neighbor_list.query_point_indices, dtype=np.int64)\n",
    "        effective_radii_per_pair = per_atom_vdw_radii_angstrom[neighbor_atom_indices] + probe_radius_angstrom\n",
    "        within_true_radius = neighbor_distances <= effective_radii_per_pair + 1e-9\n",
    "\n",
    "        # Reset & re-mark with the refined criterion\n",
    "        occupied_flags[:] = False\n",
    "        occupied_flags[neighbor_query_indices[within_true_radius]] = True\n",
    "    except Exception:\n",
    "        # If separations/distances are not exposed in your freud version,\n",
    "        # we keep the conservative occupied_flags computed above.\n",
    "        pass\n",
    "\n",
    "    ffv_estimate = float((~occupied_flags).mean())\n",
    "    return ffv_estimate\n",
    "\n",
    "per_atom_vdw_radii_angstrom = build_per_atom_vdw_radii_angstrom(u)\n",
    "\n",
    "# Example call (uses same radii array built above):\n",
    "ffv_accessible = estimate_ffv_monte_carlo(\n",
    "    freud_box=freud_box,\n",
    "    atom_positions_angstrom=positions_angstrom,\n",
    "    per_atom_vdw_radii_angstrom=per_atom_vdw_radii_angstrom,\n",
    "    # probe_radius_angstrom=1.20,   # e.g., helium-like probe; set 0.0 for geometric void\n",
    "    probe_radius_angstrom=0,\n",
    "    grid_points_per_axis=48,      # â†‘ to 64â€“96 for tighter estimates\n",
    "    rng_seed=0\n",
    ")\n",
    "\n",
    "print('FFV (monte-carlo):', ffv_accessible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668557c",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4fbb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.41651024139426, 15.065481516990053, 18.313351487763217, 24.215045194228377, 18.47992588829563, 21.166790719862608, 19.24184774307696, 16.599726512837695, 15.046631781066885, 19.62132465197348]\n",
      "Rg per chain (Ã…): median=18.86, mean=19.02, std=2.86, p10=15.06, p90=22.60, n_chains=10\n"
     ]
    }
   ],
   "source": [
    "from MDAnalysis.transformations import unwrap, wrap\n",
    "\n",
    "# 1) Make molecules whole and re-wrap into the primary cell for every frame\n",
    "# u.trajectory.add_transformations(\n",
    "#     unwrap(u.atoms),                    # join molecules split by PBC\n",
    "#     wrap(u.atoms, compound='fragments') # move each molecule back into box as a unit\n",
    "# )\n",
    "\n",
    "# 2) Compute Rg per chain (heavy atoms only), then summarize\n",
    "rg_values_per_chain = []\n",
    "for fragment in u.atoms.fragments:\n",
    "    heavy_mask = (fragment.masses > 1.2)\n",
    "    # heavy_mask = (fragment.masses > 0)\n",
    "    if np.count_nonzero(heavy_mask) < 10:\n",
    "        continue\n",
    "    chain_rg = float(fragment[heavy_mask].radius_of_gyration(wrap=True))\n",
    "    rg_values_per_chain.append(chain_rg)\n",
    "\n",
    "if not rg_values_per_chain:\n",
    "    raise RuntimeError(\"No chain had enough heavy atoms for an Rg estimate.\")\n",
    "\n",
    "print(rg_values_per_chain)\n",
    "rg_values_per_chain = np.array(rg_values_per_chain, dtype=float)\n",
    "print(\n",
    "    f\"Rg per chain (Ã…): median={np.median(rg_values_per_chain):.2f}, \"\n",
    "    f\"mean={np.mean(rg_values_per_chain):.2f}, \"\n",
    "    f\"std={np.std(rg_values_per_chain):.2f}, \"\n",
    "    f\"p10={np.percentile(rg_values_per_chain,10):.2f}, \"\n",
    "    f\"p90={np.percentile(rg_values_per_chain,90):.2f}, \"\n",
    "    f\"n_chains={len(rg_values_per_chain)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20acaedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistence length (Ã…): median=3.13, mean=3.16, n=10\n",
      "Global shape: {'asphericity_A2': 129.65323381917, 'acylindricity_A2': 75.45504106462423, 'kappa2': 0.052417042508096534, 'trace_A2': 634.1611733535154, 'lambda1_A2': 297.82254699728514, 'lambda2_A2': 205.89683371042725, 'lambda3_A2': 130.44179264580302}\n",
      "Asphericity per-chain (Ã…Â²) median: 253.17436876138657\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.transformations import unwrap, wrap, center_in_box\n",
    "from MDAnalysis.analysis import msd, polymer\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Transformations: make molecules whole & keep them in the box\n",
    "# ------------------------------------------------------------\n",
    "u.trajectory.add_transformations(\n",
    "    unwrap(u.atoms),\n",
    "    wrap(u.atoms, compound='fragments'),\n",
    "    center_in_box(u.atoms, wrap=True),\n",
    ")\n",
    "# Activate transforms by stepping the trajectory\n",
    "_ = u.trajectory[0]\n",
    "_ = u.trajectory[-1]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Diffusivity via Einstein MSD (3D): D = slope / 6\n",
    "#    Pass time_per_frame_ps if the time axis isn't present in the file.\n",
    "# ------------------------------------------------------------\n",
    "def compute_diffusivity(\n",
    "    universe: mda.Universe,\n",
    "    selection: str = \"all\",\n",
    "    fit_points: int = 20,\n",
    "    time_per_frame_ps: float | None = None\n",
    ") -> tuple[float, float]:\n",
    "    msd_result = msd.EinsteinMSD(universe, select=selection).run()\n",
    "    msd_values_A2 = np.asarray(msd_result.results.timeseries, dtype=float)\n",
    "    n_frames = len(msd_values_A2)\n",
    "    if n_frames < 2:\n",
    "        raise RuntimeError(\"Not enough frames to fit MSD slope (need >= 2 frames).\")\n",
    "\n",
    "    time_values_ps = np.asarray(msd_result.times, dtype=float)\n",
    "    if time_values_ps.size != n_frames or not np.all(np.isfinite(time_values_ps)):\n",
    "        dt_ps = time_per_frame_ps if time_per_frame_ps is not None else getattr(universe.trajectory, \"dt\", None)\n",
    "        if dt_ps is None:\n",
    "            raise RuntimeError(\"No time axis; provide time_per_frame_ps (e.g., 0.2 for 2 fs step dumped every 100).\")\n",
    "        time_values_ps = np.arange(n_frames, dtype=float) * float(dt_ps)\n",
    "\n",
    "    k = max(2, min(int(fit_points), n_frames))\n",
    "    slope_A2_per_ps = float(np.polyfit(time_values_ps[:k], msd_values_A2[:k], 1)[0])\n",
    "    diffusivity_A2_per_ps = slope_A2_per_ps / 6.0\n",
    "    diffusivity_cm2_per_s = diffusivity_A2_per_ps * 1e-4\n",
    "    return diffusivity_A2_per_ps, diffusivity_cm2_per_s\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Mass-weighted shape metrics (global and per-chain), PBC-aware\n",
    "#    Asphericity    b = Î»1 - 0.5*(Î»2 + Î»3)           (Ã…Â²)\n",
    "#    Acylindricity  c = Î»2 - Î»3                      (Ã…Â²)\n",
    "#    ÎºÂ² (anisotropy)   = 1 - 3*(Î£ Î»iÎ»j)/(Î£ Î»i)Â²      (dimensionless)\n",
    "#    We build the mass-weighted gyration tensor manually for compatibility.\n",
    "# ------------------------------------------------------------\n",
    "def mass_weighted_gyration_eigenvalues(atomgroup: mda.core.groups.AtomGroup) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return eigenvalues (Î»1 >= Î»2 >= Î»3) of the mass-weighted gyration tensor (Ã…Â²).\n",
    "    Compatible with MDAnalysis versions lacking AtomGroup.gyration_tensor().\n",
    "    \"\"\"\n",
    "    positions_angstrom = atomgroup.positions.astype(float)            # shape (N,3)\n",
    "    masses_amu = atomgroup.masses.astype(float)                       # shape (N,)\n",
    "    if positions_angstrom.size == 0:\n",
    "        raise ValueError(\"AtomGroup is empty.\")\n",
    "    if np.all(masses_amu == 0):\n",
    "        raise ValueError(\"Masses are all zero; attach masses before computing shape metrics.\")\n",
    "\n",
    "    # Mass-weighted center-of-mass in PBC (uses masses internally)\n",
    "    center_of_mass_angstrom = atomgroup.center_of_mass(wrap=True).astype(float)\n",
    "    centered_positions = positions_angstrom - center_of_mass_angstrom\n",
    "\n",
    "    # Mass-weighted gyration tensor G = (1/M) Î£ m_i r_i r_i^T\n",
    "    total_mass = float(np.sum(masses_amu))\n",
    "    weighted_positions = centered_positions * masses_amu[:, None]\n",
    "    gyration_tensor_A2 = (weighted_positions.T @ centered_positions) / total_mass  # (3,3)\n",
    "\n",
    "    # Eigenvalues sorted descending\n",
    "    eigenvalues = np.linalg.eigvalsh(gyration_tensor_A2)\n",
    "    return eigenvalues[::-1].astype(float)\n",
    "\n",
    "def shape_metrics_from_eigs(eigvals_desc: np.ndarray) -> dict:\n",
    "    lam1, lam2, lam3 = map(float, eigvals_desc)\n",
    "    trace = lam1 + lam2 + lam3\n",
    "    asphericity_A2 = lam1 - 0.5 * (lam2 + lam3)\n",
    "    acylindricity_A2 = lam2 - lam3\n",
    "    kappa2 = 1.0 - 3.0 * ((lam1*lam2 + lam2*lam3 + lam3*lam1) / (trace*trace + 1e-30))\n",
    "    return {\n",
    "        \"asphericity_A2\": asphericity_A2,\n",
    "        \"acylindricity_A2\": acylindricity_A2,\n",
    "        \"kappa2\": kappa2,\n",
    "        \"trace_A2\": trace,\n",
    "        \"lambda1_A2\": lam1,\n",
    "        \"lambda2_A2\": lam2,\n",
    "        \"lambda3_A2\": lam3,\n",
    "    }\n",
    "\n",
    "def compute_shape_metrics(universe: mda.Universe) -> tuple[dict, dict]:\n",
    "    # Global (all atoms, last frame)\n",
    "    eigvals_global = mass_weighted_gyration_eigenvalues(universe.atoms)\n",
    "    global_metrics = shape_metrics_from_eigs(eigvals_global)\n",
    "\n",
    "    # Per-chain over fragments (filter tiny ones)\n",
    "    per_chain = []\n",
    "    for fragment in universe.atoms.fragments:\n",
    "        if fragment.n_atoms < 12:\n",
    "            continue\n",
    "        eigvals = mass_weighted_gyration_eigenvalues(fragment)\n",
    "        per_chain.append(shape_metrics_from_eigs(eigvals))\n",
    "\n",
    "    summary = {}\n",
    "    if per_chain:\n",
    "        keys = per_chain[0].keys()\n",
    "        for key in keys:\n",
    "            values = np.array([d[key] for d in per_chain], dtype=float)\n",
    "            summary[key] = {\n",
    "                \"median\": float(np.median(values)),\n",
    "                \"mean\": float(np.mean(values)),\n",
    "                \"std\": float(np.std(values)),\n",
    "                \"p10\": float(np.percentile(values, 10)),\n",
    "                \"p90\": float(np.percentile(values, 90)),\n",
    "                \"n_chains\": int(len(values)),\n",
    "            }\n",
    "    return global_metrics, summary\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Persistence length from heavy-atom backbone paths (uses bonds)\n",
    "#    Same graph-based backbone finder you used earlier.\n",
    "# ------------------------------------------------------------\n",
    "def _build_adjacency_from_bonds(n_atoms: int, bonds_0based: np.ndarray) -> list[list[int]]:\n",
    "    adjacency = [[] for _ in range(n_atoms)]\n",
    "    for ai, aj in bonds_0based:\n",
    "        adjacency[ai].append(aj)\n",
    "        adjacency[aj].append(ai)\n",
    "    return adjacency\n",
    "\n",
    "def _bfs_path(adjacency: list[list[int]], start: int, goal: int) -> list[int]:\n",
    "    queue = deque([start]); parent = {start: None}\n",
    "    while queue:\n",
    "        v = queue.popleft()\n",
    "        if v == goal:\n",
    "            break\n",
    "        for w in adjacency[v]:\n",
    "            if w not in parent:\n",
    "                parent[w] = v\n",
    "                queue.append(w)\n",
    "    if goal not in parent:\n",
    "        return []\n",
    "    path = []\n",
    "    cur = goal\n",
    "    while cur is not None:\n",
    "        path.append(cur); cur = parent[cur]\n",
    "    return path[::-1]\n",
    "\n",
    "def _double_bfs_longest_path(adjacency: list[list[int]], nodes: list[int]) -> list[int]:\n",
    "    node_set = set(nodes)\n",
    "    def farthest(x: int) -> tuple[int, dict[int, int | None]]:\n",
    "        q = deque([x]); parent = {x: None}; last = x\n",
    "        while q:\n",
    "            v = q.popleft(); last = v\n",
    "            for w in adjacency[v]:\n",
    "                if w not in parent and w in node_set:\n",
    "                    parent[w] = v; q.append(w)\n",
    "        return last, parent\n",
    "    a = nodes[0]\n",
    "    a, _ = farthest(a)\n",
    "    b, parent = farthest(a)\n",
    "    path = []\n",
    "    cur = b\n",
    "    while cur is not None:\n",
    "        path.append(cur); cur = parent[cur]\n",
    "    path.reverse()\n",
    "    return [v for v in path if v in node_set]\n",
    "\n",
    "def build_backbone_paths_heavy_atoms(universe: mda.Universe, min_atoms_per_chain: int = 12) -> list[mda.core.groups.AtomGroup]:\n",
    "    assert getattr(universe, \"bonds\", None) is not None and len(universe.bonds) > 0, \"Universe has no bonds; attach them first.\"\n",
    "    heavy_mask = (universe.atoms.masses > 1.2)\n",
    "    heavy_indices = set(np.nonzero(heavy_mask)[0].tolist())\n",
    "    adjacency = _build_adjacency_from_bonds(len(universe.atoms), universe.bonds.to_indices())\n",
    "\n",
    "    # connected components within heavy subgraph\n",
    "    unvisited = set(heavy_indices)\n",
    "    chains = []\n",
    "    while unvisited:\n",
    "        seed = next(iter(unvisited))\n",
    "        component = []\n",
    "        q = deque([seed]); unvisited.remove(seed)\n",
    "        while q:\n",
    "            v = q.popleft(); component.append(v)\n",
    "            for w in adjacency[v]:\n",
    "                if w in unvisited:\n",
    "                    unvisited.remove(w); q.append(w)\n",
    "\n",
    "        # endpoints in heavy subgraph\n",
    "        degree = {v: sum((nbr in component) for nbr in adjacency[v]) for v in component}\n",
    "        endpoints = [v for v in component if degree[v] == 1]\n",
    "        if len(endpoints) >= 2:\n",
    "            best = []\n",
    "            for i in range(len(endpoints)):\n",
    "                for j in range(i + 1, len(endpoints)):\n",
    "                    path = _bfs_path(adjacency, endpoints[i], endpoints[j])\n",
    "                    path = [p for p in path if p in set(component)]\n",
    "                    if len(path) > len(best):\n",
    "                        best = path\n",
    "            path_indices = best\n",
    "        else:\n",
    "            path_indices = _double_bfs_longest_path(adjacency, component)\n",
    "\n",
    "        if len(path_indices) >= min_atoms_per_chain:\n",
    "            chains.append(universe.atoms[path_indices])\n",
    "    return chains\n",
    "\n",
    "def compute_persistence_length_stats(universe: mda.Universe, backbone_chains: list[mda.core.groups.AtomGroup]) -> dict:\n",
    "    per_chain_lp = []\n",
    "    for chain_ag in backbone_chains:\n",
    "        try:\n",
    "            pl_calc = polymer.PersistenceLength([chain_ag]).run()\n",
    "            lp_value = float(np.nanmean(pl_calc.results.lp))\n",
    "            if np.isfinite(lp_value):\n",
    "                per_chain_lp.append(lp_value)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not per_chain_lp:\n",
    "        raise RuntimeError(\"Persistence length failed for all chains.\")\n",
    "    arr = np.array(per_chain_lp, dtype=float)\n",
    "    return {\n",
    "        \"median\": float(np.median(arr)),\n",
    "        \"mean\": float(np.mean(arr)),\n",
    "        \"std\": float(np.std(arr)),\n",
    "        \"p10\": float(np.percentile(arr, 10)),\n",
    "        \"p90\": float(np.percentile(arr, 90)),\n",
    "        \"n_chains\": int(len(arr)),\n",
    "    }\n",
    "\n",
    "\n",
    "# D_A2ps, D_cm2s = compute_diffusivity(u, selection=\"all\", fit_points=20, time_per_frame_ps=0.2)\n",
    "backbone_chains = build_backbone_paths_heavy_atoms(u, min_atoms_per_chain=12)\n",
    "pl_stats = compute_persistence_length_stats(u, backbone_chains)\n",
    "print(f\"Persistence length (Ã…): median={pl_stats['median']:.2f}, mean={pl_stats['mean']:.2f}, n={pl_stats['n_chains']}\")\n",
    "\n",
    "global_shape, per_chain_shape = compute_shape_metrics(u)\n",
    "print(\"Global shape:\", global_shape)\n",
    "if per_chain_shape:\n",
    "    print(\"Asphericity per-chain (Ã…Â²) median:\", per_chain_shape[\"asphericity_A2\"][\"median\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
