{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b9ef3c",
   "metadata": {},
   "source": [
    "# Drop nans for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae72d125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>SMILES</th><th>Tg</th><th>FFV</th><th>Tc</th><th>Density</th><th>Rg</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>87817</td><td>&quot;*CC(*)c1ccccc1C(=O)OCCCCCC&quot;</td><td>null</td><td>0.374645</td><td>0.205667</td><td>null</td><td>null</td></tr><tr><td>106919</td><td>&quot;*Nc1ccc([C@H](CCC)c2ccc(C3(c4c…</td><td>null</td><td>0.3704102</td><td>null</td><td>null</td><td>null</td></tr><tr><td>388772</td><td>&quot;*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(…</td><td>null</td><td>0.37886</td><td>null</td><td>null</td><td>null</td></tr><tr><td>519416</td><td>&quot;*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c…</td><td>null</td><td>0.3873239</td><td>null</td><td>null</td><td>null</td></tr><tr><td>539187</td><td>&quot;*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCO…</td><td>null</td><td>0.35547</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌────────┬─────────────────────────────────┬──────┬───────────┬──────────┬─────────┬──────┐\n",
       "│ id     ┆ SMILES                          ┆ Tg   ┆ FFV       ┆ Tc       ┆ Density ┆ Rg   │\n",
       "│ ---    ┆ ---                             ┆ ---  ┆ ---       ┆ ---      ┆ ---     ┆ ---  │\n",
       "│ i64    ┆ str                             ┆ f64  ┆ f64       ┆ f64      ┆ f64     ┆ f64  │\n",
       "╞════════╪═════════════════════════════════╪══════╪═══════════╪══════════╪═════════╪══════╡\n",
       "│ 87817  ┆ *CC(*)c1ccccc1C(=O)OCCCCCC      ┆ null ┆ 0.374645  ┆ 0.205667 ┆ null    ┆ null │\n",
       "│ 106919 ┆ *Nc1ccc([C@H](CCC)c2ccc(C3(c4c… ┆ null ┆ 0.3704102 ┆ null     ┆ null    ┆ null │\n",
       "│ 388772 ┆ *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(… ┆ null ┆ 0.37886   ┆ null     ┆ null    ┆ null │\n",
       "│ 519416 ┆ *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c… ┆ null ┆ 0.3873239 ┆ null     ┆ null    ┆ null │\n",
       "│ 539187 ┆ *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCO… ┆ null ┆ 0.35547   ┆ null     ┆ null    ┆ null │\n",
       "└────────┴─────────────────────────────────┴──────┴───────────┴──────────┴─────────┴──────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "host_df = pl.read_csv('../../data/from_host/train.csv')\n",
    "host_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1161fd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>SMILES</th><th>TARGET</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;*NC(C)C(=O)NCC(=O)NCC(*)=O&quot;</td><td>208.639749</td></tr><tr><td>&quot;*CCCCCCSSCCCCSS*&quot;</td><td>-41.266724</td></tr><tr><td>&quot;*C=CCCCCCCCC*&quot;</td><td>-17.282022</td></tr><tr><td>&quot;*CCCCCCCCCCOC(=O)c1ccc(C(=O)NC…</td><td>4.250403</td></tr><tr><td>&quot;*c1nc2cc3sc(-c4cc(OCCCCCC)c(*)…</td><td>168.526313</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬────────────┐\n",
       "│ SMILES                          ┆ TARGET     │\n",
       "│ ---                             ┆ ---        │\n",
       "│ str                             ┆ f64        │\n",
       "╞═════════════════════════════════╪════════════╡\n",
       "│ *NC(C)C(=O)NCC(=O)NCC(*)=O      ┆ 208.639749 │\n",
       "│ *CCCCCCSSCCCCSS*                ┆ -41.266724 │\n",
       "│ *C=CCCCCCCCC*                   ┆ -17.282022 │\n",
       "│ *CCCCCCCCCCOC(=O)c1ccc(C(=O)NC… ┆ 4.250403   │\n",
       "│ *c1nc2cc3sc(-c4cc(OCCCCCC)c(*)… ┆ 168.526313 │\n",
       "└─────────────────────────────────┴────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_tg_df = host_df.drop_nulls(subset='Tg').with_columns(\n",
    "    pl.col('Tg').alias('TARGET')\n",
    ")['SMILES', 'TARGET']\n",
    "\n",
    "host_tg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7102e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Conformer at 0x7fb48f8d3d10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unimol_tools.data.conformer import mol2unimolv2\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "mol = Chem.MolFromSmiles(\"*C=CCCCCCCCC*\")\n",
    "# mol2unimolv2(mol)\n",
    "\n",
    "AllChem.EmbedMolecule(mol)\n",
    "\n",
    "mol.GetConformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258ed64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:44<00:00, 104.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "# TARGET_NAMES= [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "TARGET_NAMES= [\"FFV\"]\n",
    "\n",
    "\n",
    "def can_embed(smiles_string: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True only if RDKit can parse the SMILES *and*\n",
    "    `AllChem.EmbedMolecule` succeeds (status == 0).\n",
    "\n",
    "    Any parsing, sanitisation, or embedding error ⇒ False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        molecule = Chem.MolFromSmiles(smiles_string)\n",
    "\n",
    "        if molecule.GetNumAtoms(onlyExplicit=False) > 130:\n",
    "            return False\n",
    "\n",
    "        if molecule is None:\n",
    "            return False                          # unparsable SMILES\n",
    "        # 1‑shot, quick‑fail embed; tweak kwargs if you like\n",
    "        embed_status: int = AllChem.EmbedMolecule(\n",
    "            molecule,\n",
    "            maxAttempts=10,\n",
    "            clearConfs=True,\n",
    "        )\n",
    "        return embed_status == 0\n",
    "    except Exception:                             # catches RDKit C++ errors too\n",
    "        # traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "for target_name in tqdm(TARGET_NAMES):\n",
    "    subset_df = (\n",
    "        host_df#.sample(n=100)\n",
    "        .drop_nulls(subset=target_name)\n",
    "        .with_columns(\n",
    "            pl.col(target_name).alias('TARGET')\n",
    "        )\n",
    "        .filter(\n",
    "            pl.col(\"SMILES\").map_elements(        # keeps only embeddable rows\n",
    "                can_embed,\n",
    "                return_dtype=pl.Boolean,\n",
    "            )\n",
    "            # # Only keep rows whose SMILES can be embedded (EmbedMolecule == 0)\n",
    "            # pl.col(\"SMILES\").map_elements(\n",
    "            #     lambda smiles_string: (\n",
    "            #         (molecule := Chem.MolFromSmiles(smiles_string)) is not None\n",
    "            #         and AllChem.EmbedMolecule(molecule) == 0\n",
    "            #     ),\n",
    "            #     return_dtype=pl.Boolean,\n",
    "            # )\n",
    "        )\n",
    "        ['SMILES', 'TARGET']\n",
    "    )\n",
    "    subset_df.write_csv(f'from_host/{target_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b47e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing targets: 100%|██████████| 5/5 [01:44<00:00, 20.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "TARGET_NAMES: list[str] = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "OUTPUT_DIR: Path = Path(\"from_host\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def build_smiles_processor(target_name: str) -> Callable[[str], str | None]:\n",
    "    \"\"\"\n",
    "    Return a function that:\n",
    "      • parses a SMILES,\n",
    "      • replaces '*' (atomic number 0) with carbon,\n",
    "      • canonicalises and embeds it,\n",
    "      • applies optional size filter for FFV,\n",
    "      • and returns the canonical SMILES *or* None on failure.\n",
    "    \"\"\"\n",
    "\n",
    "    def _process(original_smiles: str) -> str | None:\n",
    "        try:\n",
    "            # ── 1  parse without sanitising first ────────────────────────────\n",
    "            molecule = Chem.MolFromSmiles(original_smiles, sanitize=False)\n",
    "            if molecule is None:  # unparsable string\n",
    "                return None\n",
    "\n",
    "            # ── 2  replace wildcards with carbon ────────────────────────────\n",
    "            for atom in molecule.GetAtoms():\n",
    "                if atom.GetAtomicNum() == 0:\n",
    "                    # atom.SetAtomicNum(6)\n",
    "                    atom.SetAtomicNum(85)\n",
    "\n",
    "            # ── 3  full sanitisation ────────────────────────────────────────\n",
    "            Chem.SanitizeMol(molecule)\n",
    "\n",
    "            # ── 4  dataset‑specific filter ──────────────────────────────────\n",
    "            if (\n",
    "                target_name == \"FFV\"\n",
    "                and molecule.GetNumAtoms(onlyExplicit=False) > 110\n",
    "            ):\n",
    "                return None\n",
    "\n",
    "            # ── 5  conformer generation sanity check ───────────────────────\n",
    "            embed_status: int = AllChem.EmbedMolecule(\n",
    "                molecule, maxAttempts=5, clearConfs=True\n",
    "            )\n",
    "            if embed_status != 0:\n",
    "                return None\n",
    "\n",
    "            # ── 6  return canonicalised SMILES ──────────────────────────────\n",
    "            return Chem.MolToSmiles(\n",
    "                molecule, canonical=True, isomericSmiles=True\n",
    "            )\n",
    "\n",
    "        except Exception:\n",
    "            # Anything weird → drop the row\n",
    "            return None\n",
    "\n",
    "    return _process\n",
    "\n",
    "\n",
    "for target_name in tqdm(TARGET_NAMES, desc=\"processing targets\"):\n",
    "    preprocess_and_embed = build_smiles_processor(target_name)\n",
    "\n",
    "    subset_df: pl.DataFrame = (\n",
    "        host_df\n",
    "        # keep only rows with a label for this target\n",
    "        .drop_nulls(subset=target_name)\n",
    "        # rename the column to the generic name expected downstream\n",
    "        .with_columns(pl.col(target_name).alias(\"TARGET\"))\n",
    "        # convert SMILES → canonicalised; invalid rows become null\n",
    "        .with_columns(\n",
    "            pl.col(\"SMILES\")\n",
    "            .map_elements(preprocess_and_embed, return_dtype=pl.Utf8)\n",
    "            .alias(\"SMILES\")\n",
    "        )\n",
    "        # remove rows that failed preprocessing\n",
    "        .drop_nulls(subset=[\"SMILES\"])\n",
    "        # output only what Uni‑Mol 2 needs\n",
    "        .select([\"SMILES\", \"TARGET\"])\n",
    "    )\n",
    "\n",
    "    subset_df.write_csv(OUTPUT_DIR / f\"{target_name}_At.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b4cc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 101, 78, 48, 40]\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from rdkit import Chem\n",
    "\n",
    "tg_smiles = pl.read_csv('from_host/FFV.csv')['SMILES']\n",
    "atom_counts = [\n",
    "    Chem.MolFromSmiles(smi).GetNumAtoms(onlyExplicit=False)\n",
    "    for smi in tg_smiles\n",
    "]\n",
    "print(atom_counts[:5])\n",
    "print(max(atom_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9f7fa",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ba3bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 23:06:31 | unimol_tools/data/conformer.py | 437 | INFO | Uni-Mol Tools | Start generating conformers...\n",
      "410it [00:04, 92.45it/s] \n",
      "2025-07-15 23:06:35 | unimol_tools/data/conformer.py | 452 | INFO | Uni-Mol Tools | Succeeded in generating conformers for 100.00% of molecules.\n",
      "2025-07-15 23:06:35 | unimol_tools/data/conformer.py | 469 | INFO | Uni-Mol Tools | Succeeded in generating 3d conformers for 100.00% of molecules.\n",
      "2025-07-15 23:06:35 | unimol_tools/data/datahub.py | 181 | INFO | Uni-Mol Tools | conf_cache_level is 2, saving conformers to ./exp/Tg.sdf.\n",
      "2025-07-15 23:06:36 | unimol_tools/data/datahub.py | 195 | INFO | Uni-Mol Tools | Successfully saved sdf file to ./exp/Tg.sdf\n",
      "2025-07-15 23:06:36 | unimol_tools/data/datahub.py | 146 | INFO | Uni-Mol Tools | Split method: random, fold: 5\n",
      "2025-07-15 23:06:36 | unimol_tools/train.py | 223 | INFO | Uni-Mol Tools | Output directory already exists: ./exp\n",
      "2025-07-15 23:06:36 | unimol_tools/train.py | 226 | INFO | Uni-Mol Tools | Warning: Overwrite output directory: ./exp\n",
      "2025-07-15 23:06:36 | unimol_tools/tasks/trainer.py | 78 | INFO | Uni-Mol Tools | Number of GPUs available: 2\n",
      "2025-07-15 23:06:36 | unimol_tools/tasks/trainer.py | 98 | INFO | Uni-Mol Tools | Using single GPU.\n",
      "2025-07-15 23:06:36 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from /home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/unimol_tools/weights/modelzoo/84M/checkpoint.pt\n",
      "2025-07-15 23:06:36 | unimol_tools/models/nnmodel.py | 174 | INFO | Uni-Mol Tools | start training Uni-Mol:unimolv2\n",
      "2025-07-15 23:06:47 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [1/10] train_loss: 80.2620, val_loss: 3.9787, val_mae: 193.3855, lr: 0.000093\n",
      "2025-07-15 23:06:57 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [2/10] train_loss: 5.2354, val_loss: 1.7998, val_mae: 135.7771, lr: 0.000082\n",
      "2025-07-15 23:07:07 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [3/10] train_loss: 3.1282, val_loss: 0.5118, val_mae: 63.5636, lr: 0.000072\n",
      "2025-07-15 23:07:18 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [4/10] train_loss: 1.4997, val_loss: 0.8126, val_mae: 89.8795, lr: 0.000062\n",
      "2025-07-15 23:07:28 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [5/10] train_loss: 1.6352, val_loss: 2.4059, val_mae: 171.5185, lr: 0.000052\n",
      "2025-07-15 23:07:37 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [6/10] train_loss: 1.1532, val_loss: 0.3678, val_mae: 60.6264, lr: 0.000041\n",
      "2025-07-15 23:07:48 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [7/10] train_loss: 1.9979, val_loss: 0.6036, val_mae: 77.5657, lr: 0.000031\n",
      "2025-07-15 23:07:58 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [8/10] train_loss: 0.9012, val_loss: 1.4003, val_mae: 117.8734, lr: 0.000021\n",
      "2025-07-15 23:08:08 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [9/10] train_loss: 1.0291, val_loss: 0.4506, val_mae: 59.9054, lr: 0.000010\n",
      "2025-07-15 23:08:18 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [10/10] train_loss: 0.6347, val_loss: 0.3501, val_mae: 56.4780, lr: 0.000000\n",
      "2025-07-15 23:08:19 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from ./exp/model_0.pth\n",
      "2025-07-15 23:08:20 | unimol_tools/tasks/trainer.py | 587 | INFO | Uni-Mol Tools | load model success!\n",
      "2025-07-15 23:08:21 | unimol_tools/models/nnmodel.py | 240 | INFO | Uni-Mol Tools | fold 0, result {'mae': 56.47798538208008, 'pearsonr': 0.7656933, 'spearmanr': 0.6877047485334291, 'mse': 5146.740234375, 'r2': 0.4717971682548523}\n",
      "2025-07-15 23:08:22 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from /home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/unimol_tools/weights/modelzoo/84M/checkpoint.pt\n",
      "2025-07-15 23:08:32 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [1/10] train_loss: 16.0931, val_loss: 5.9144, val_mae: 261.7549, lr: 0.000093\n",
      "2025-07-15 23:08:43 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [2/10] train_loss: 4.7272, val_loss: 3.7605, val_mae: 209.3148, lr: 0.000082\n",
      "2025-07-15 23:08:54 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [3/10] train_loss: 3.0728, val_loss: 3.0212, val_mae: 190.3507, lr: 0.000072\n",
      "2025-07-15 23:09:05 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [4/10] train_loss: 1.4954, val_loss: 3.2260, val_mae: 189.8619, lr: 0.000062\n",
      "2025-07-15 23:09:16 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [5/10] train_loss: 1.5801, val_loss: 1.3865, val_mae: 109.2607, lr: 0.000052\n",
      "2025-07-15 23:09:27 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [6/10] train_loss: 0.9853, val_loss: 0.4684, val_mae: 64.3109, lr: 0.000041\n",
      "2025-07-15 23:09:38 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [7/10] train_loss: 1.4711, val_loss: 0.7151, val_mae: 81.7184, lr: 0.000031\n",
      "2025-07-15 23:09:48 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [8/10] train_loss: 0.8656, val_loss: 0.3718, val_mae: 55.6395, lr: 0.000021\n",
      "2025-07-15 23:09:59 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [9/10] train_loss: 0.8862, val_loss: 0.7886, val_mae: 85.8118, lr: 0.000010\n",
      "2025-07-15 23:10:09 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [10/10] train_loss: 0.8693, val_loss: 0.5750, val_mae: 66.1261, lr: 0.000000\n",
      "2025-07-15 23:10:09 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from ./exp/model_1.pth\n",
      "2025-07-15 23:10:09 | unimol_tools/tasks/trainer.py | 587 | INFO | Uni-Mol Tools | load model success!\n",
      "2025-07-15 23:10:11 | unimol_tools/models/nnmodel.py | 240 | INFO | Uni-Mol Tools | fold 1, result {'mae': 55.63948059082031, 'pearsonr': 0.8085289, 'spearmanr': 0.7890749991837267, 'mse': 5266.67431640625, 'r2': 0.6510494947433472}\n",
      "2025-07-15 23:10:11 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from /home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/unimol_tools/weights/modelzoo/84M/checkpoint.pt\n",
      "2025-07-15 23:10:22 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [1/10] train_loss: 33.1453, val_loss: 1.9552, val_mae: 113.1555, lr: 0.000093\n",
      "2025-07-15 23:10:33 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [2/10] train_loss: 6.0260, val_loss: 1.0041, val_mae: 82.4340, lr: 0.000082\n",
      "2025-07-15 23:10:43 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [3/10] train_loss: 1.9295, val_loss: 0.5938, val_mae: 72.9531, lr: 0.000072\n",
      "2025-07-15 23:10:54 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [4/10] train_loss: 2.8762, val_loss: 3.1676, val_mae: 187.2857, lr: 0.000062\n",
      "2025-07-15 23:11:03 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [5/10] train_loss: 3.2235, val_loss: 1.1510, val_mae: 106.1012, lr: 0.000052\n",
      "2025-07-15 23:11:13 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [6/10] train_loss: 1.2677, val_loss: 0.5912, val_mae: 69.0119, lr: 0.000041\n",
      "2025-07-15 23:11:24 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [7/10] train_loss: 0.7286, val_loss: 0.7145, val_mae: 78.7015, lr: 0.000031\n",
      "2025-07-15 23:11:34 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [8/10] train_loss: 1.0599, val_loss: 0.6697, val_mae: 75.1160, lr: 0.000021\n",
      "2025-07-15 23:11:44 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [9/10] train_loss: 0.8948, val_loss: 0.2895, val_mae: 49.5462, lr: 0.000010\n",
      "2025-07-15 23:11:55 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [10/10] train_loss: 0.7382, val_loss: 0.2935, val_mae: 50.4801, lr: 0.000000\n",
      "2025-07-15 23:11:55 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from ./exp/model_2.pth\n",
      "2025-07-15 23:11:55 | unimol_tools/tasks/trainer.py | 587 | INFO | Uni-Mol Tools | load model success!\n",
      "2025-07-15 23:11:57 | unimol_tools/models/nnmodel.py | 240 | INFO | Uni-Mol Tools | fold 2, result {'mae': 49.546234130859375, 'pearsonr': 0.8899187, 'spearmanr': 0.873945647086993, 'mse': 3920.618408203125, 'r2': 0.7576528787612915}\n",
      "2025-07-15 23:11:57 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from /home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/unimol_tools/weights/modelzoo/84M/checkpoint.pt\n",
      "2025-07-15 23:12:08 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [1/10] train_loss: 29.3193, val_loss: 2.4849, val_mae: 169.6254, lr: 0.000093\n",
      "2025-07-15 23:12:19 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [2/10] train_loss: 6.9406, val_loss: 9.5813, val_mae: 352.1272, lr: 0.000082\n",
      "2025-07-15 23:12:29 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [3/10] train_loss: 4.3098, val_loss: 3.7353, val_mae: 207.5198, lr: 0.000072\n",
      "2025-07-15 23:12:39 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [4/10] train_loss: 2.5172, val_loss: 0.8165, val_mae: 81.6726, lr: 0.000062\n",
      "2025-07-15 23:12:50 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [5/10] train_loss: 1.5782, val_loss: 2.3793, val_mae: 149.4134, lr: 0.000052\n",
      "2025-07-15 23:13:00 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [6/10] train_loss: 2.4054, val_loss: 1.1959, val_mae: 106.2288, lr: 0.000041\n",
      "2025-07-15 23:13:10 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [7/10] train_loss: 1.8920, val_loss: 0.9089, val_mae: 91.9721, lr: 0.000031\n",
      "2025-07-15 23:13:21 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [8/10] train_loss: 1.0839, val_loss: 0.5991, val_mae: 74.2679, lr: 0.000021\n",
      "2025-07-15 23:13:32 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [9/10] train_loss: 0.9212, val_loss: 0.3508, val_mae: 56.5129, lr: 0.000010\n",
      "2025-07-15 23:13:43 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [10/10] train_loss: 0.7419, val_loss: 0.3594, val_mae: 53.1600, lr: 0.000000\n",
      "2025-07-15 23:13:44 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from ./exp/model_3.pth\n",
      "2025-07-15 23:13:44 | unimol_tools/tasks/trainer.py | 587 | INFO | Uni-Mol Tools | load model success!\n",
      "2025-07-15 23:13:46 | unimol_tools/models/nnmodel.py | 240 | INFO | Uni-Mol Tools | fold 3, result {'mae': 53.15999984741211, 'pearsonr': 0.78065044, 'spearmanr': 0.7516135000707438, 'mse': 4578.92919921875, 'r2': 0.6018955707550049}\n",
      "2025-07-15 23:13:46 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from /home/jday/anaconda3/envs/torch2.7/lib/python3.12/site-packages/unimol_tools/weights/modelzoo/84M/checkpoint.pt\n",
      "2025-07-15 23:13:57 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [1/10] train_loss: 17.6657, val_loss: 1.8694, val_mae: 97.5965, lr: 0.000093\n",
      "2025-07-15 23:14:08 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [2/10] train_loss: 5.6613, val_loss: 0.8412, val_mae: 76.6102, lr: 0.000082\n",
      "2025-07-15 23:14:19 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [3/10] train_loss: 2.6363, val_loss: 4.1922, val_mae: 205.6289, lr: 0.000072\n",
      "2025-07-15 23:14:29 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [4/10] train_loss: 2.0609, val_loss: 0.8311, val_mae: 71.7870, lr: 0.000062\n",
      "2025-07-15 23:14:40 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [5/10] train_loss: 1.7256, val_loss: 0.6089, val_mae: 58.7948, lr: 0.000052\n",
      "2025-07-15 23:14:52 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [6/10] train_loss: 1.4683, val_loss: 0.7368, val_mae: 65.4232, lr: 0.000041\n",
      "2025-07-15 23:15:02 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [7/10] train_loss: 0.6913, val_loss: 0.7442, val_mae: 67.0711, lr: 0.000031\n",
      "2025-07-15 23:15:12 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [8/10] train_loss: 0.8435, val_loss: 0.6950, val_mae: 66.4326, lr: 0.000021\n",
      "2025-07-15 23:15:22 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [9/10] train_loss: 0.8427, val_loss: 0.7649, val_mae: 70.2183, lr: 0.000010\n",
      "2025-07-15 23:15:33 | unimol_tools/tasks/trainer.py | 505 | INFO | Uni-Mol Tools | Epoch [10/10] train_loss: 0.5760, val_loss: 0.6679, val_mae: 64.1615, lr: 0.000000\n",
      "2025-07-15 23:15:33 | unimol_tools/models/unimolv2.py | 161 | INFO | Uni-Mol Tools | Loading pretrained weights from ./exp/model_4.pth\n",
      "2025-07-15 23:15:33 | unimol_tools/tasks/trainer.py | 587 | INFO | Uni-Mol Tools | load model success!\n",
      "2025-07-15 23:15:34 | unimol_tools/models/nnmodel.py | 240 | INFO | Uni-Mol Tools | fold 4, result {'mae': 58.794803619384766, 'pearsonr': 0.6555613, 'spearmanr': 0.6625417659799088, 'mse': 6698.61572265625, 'r2': 0.3615897297859192}\n",
      "2025-07-15 23:15:34 | unimol_tools/models/nnmodel.py | 258 | INFO | Uni-Mol Tools | Uni-Mol metrics score: \n",
      "{'mae': 54.72370237721238, 'pearsonr': 0.7795227932409188, 'spearmanr': 0.7459312521845795, 'mse': 5122.315569984664, 'r2': 0.5991287588357533}\n",
      "2025-07-15 23:15:34 | unimol_tools/models/nnmodel.py | 259 | INFO | Uni-Mol Tools | Uni-Mol & Metric result saved!\n"
     ]
    }
   ],
   "source": [
    "from unimol_tools import MolTrain, MolPredict\n",
    "\n",
    "clf = MolTrain(\n",
    "    task='regression', \n",
    "    data_type='molecule', \n",
    "    epochs=10, \n",
    "    learning_rate=1e-4,\n",
    "    batch_size=16, \n",
    "    kfold=5,\n",
    "    model_name='unimolv2',\n",
    "    model_size='84m',\n",
    "    early_stopping=1e9,\n",
    "    metrics='mae',\n",
    "    conf_cache_level=2,\n",
    "    save_path='./exp'\n",
    ")\n",
    "clf.fit(data = 'from_host/Tg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8509188c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 54.72370237721238,\n",
       " 'pearsonr': 0.7795227932409188,\n",
       " 'spearmanr': 0.7459312521845795,\n",
       " 'mse': 5122.315569984664,\n",
       " 'r2': 0.5991287588357533}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.load('exp/metric.result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1461a12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 0.022057177832748225,\n",
       " 'pearsonr': 0.901795441267618,\n",
       " 'spearmanr': 0.925422367748698,\n",
       " 'mse': 0.001351675419052299,\n",
       " 'r2': 0.8124799961411993}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.load('../../models/UniMol2_2025_07_16/Tc/metric.result')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
